{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVCkXWAg3kys63RSNBQGDk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saabir2003/pwskills/blob/main/Regression_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) What is Simple Linear Regression?\n",
        "\n",
        "A simple linear regression has one independent variable and one dependent variable it is usually characteristics by the equation y is equal to MXC generally a sin thermal linear regression is used when we want a given output or to calculate a given output it only has 1M and one C which means one slope and one intercept which can be easily calculated using mean square or mean absolute error this can be used to create a best fit line for the data points this best fit line can be used to calculate the future outputs"
      ],
      "metadata": {
        "id": "bX_GyvjObmcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2)- What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "There are few main assumptions of the  simple Rainier regression some of the few assumptions are assume all the data is linear the second assumption is we assume the error along the best pit line is normally distributed another assumption is homosexity this means the variance of error is constant the feature should not be related or should have least relation that means it should have a least multicolinearity the independence or observation should be independent of each other and the errors should also be independents\n",
        "\n",
        "X and Y should have a linear relationship that means direct and indirect relationship of the input and output variable should have a linear relationship among themselves\n",
        "\n",
        "The independence observers are independent of each other when the observer has independent then error is also independent\n",
        "\n",
        "Homoseducicity :- It means the variance of the error should be the constant along the best fit line\n",
        "\n",
        "Normality of the errors the all errors should be normally distributed among a linear line which can be also a best fitted line\n",
        "\n",
        "\n",
        "The features should not be related to each other and should have a least relationship that is multicolinearity shouldnt exist if multicolor linearity exists then the elimination of the feature should be done using RFP recursive feature elimination vif here variatie inflation factor"
      ],
      "metadata": {
        "id": "GwU9jWpQb9Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "The coefficient M result in the equation is a slope of the equation it is also known as the tan theta it refers to the steepness of the angle of the best fitted line with respect to the X axis\n",
        "\n",
        "it can range from zero to one between zero to 45 degrees and one to Infinity between 45 to 90 degrees"
      ],
      "metadata": {
        "id": "qR5EjX3Ncwoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4)What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "The intercept seal present in the equation usually refers to the cut when X is equal to zero it is a predicted output when X is equal to input or X is equal to 0 it is usually considered as bias and a helpful in moving the best fitted line along the Y axis this can be used to accurately predict the output data"
      ],
      "metadata": {
        "id": "C1VOt-XrdA3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5)How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "The two ways to calculate the slope in linear regression one method is to tape two points and calculate Y 2 - Y 1 by X 2 - X 1 this is a 1 method the second method is we can take a random line and calculate the mean absolute error and account to the mean absolute error we can take step by step and iterate through a process to get the best fit line after getting a best fit line the slope of the M can be calculated or can be taken easily rho is equal to"
      ],
      "metadata": {
        "id": "YY4kaUc7dPha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6)What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "What is the purpose of Lees Square noted in linear regression is the least square method generally takes error and squares it this ensures that all the negative and posteriors are converted into the positive errors and larger errors are penalized more heavily than the smaller errors this can be used to calculate the cost function which should be partially differentiated to adjust the best fit line accordingly"
      ],
      "metadata": {
        "id": "PrVaPfcseuk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "\n",
        "The cohesion determination R Square is anticipated in lead simple linear progression the following ways it is generally calculated when we divide explained variance by total variance explain variance generally means that the natural variance of the data points along the best fit line the total variance means sum of the explained variance and explain variance unexplained variance means error in the actual line explain variance means general error compared to the mean of the data points it can be calculated using one minus unexplained error by total error or explain variance by total variance"
      ],
      "metadata": {
        "id": "BhRmtboDe8gW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) What is Multiple Linear Regression?\n",
        "\n",
        "Alex simple linear regression multiple linear regression has multip le independent variables or inputs this can result in multiple slopes such as M1 M2 M3 and etc this can be more complex and should be visualized in more than three dimensional graphs which is not possible but multiple linear regression can be calculated mathematically and be used by the computers to visualize in the calculate the outputs accordingly"
      ],
      "metadata": {
        "id": "J3_P_vMUfUcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9)What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "the main difference between simple linear relation multiple linear regression simple linear regression has only one independent variable and 1 dependent variable that is it has only one input and 1 onward whereas multiple linear regression can have multiple inputs under single output a simple linear regression is easy to compute and generally can be visualized in an XY plane in multiple regression it may not be possible to visualize the best fit line because when the number of inputs increase from three then visualizing it in a graph is impossible but it can be a bit can be mathematically computed using various equations these equations should be calculated accordingly"
      ],
      "metadata": {
        "id": "XuG0znR0fr3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10)What are the key assumptions of Multiple Linear Regression?\n",
        "The main key assumptions of the multiple linear regression is all the output data points are linear the second is there is no multicolinearity between the two inputs if there is any multi cooling ert it has to be eliminated using rfe recursive feature elimination rbif variance inflation factor the third thing is all the errors are normalized and non normally distributed along the best fat line this can be also called as explained variance the 4th thing is there has to be multiple data points in a huge amount of data to calculate multiple coefficients of input variables accordingly.\n",
        "\n",
        "No multi-collinearity between the variables\n",
        "\n",
        "Homosedacity city the variance of the errors are constant\n",
        "\n",
        "linearity all the outputs of the results are linear\n",
        "\n",
        "Independence all the input variables are independent of each other\n",
        "\n",
        "There should be enough independence between the input variables and the output variables"
      ],
      "metadata": {
        "id": "YVQCq7TKf-n_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11) What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Heteroscedasticity generally means the variance generally varies from one output variables to another output variable this can affect the results of multiple linear regression as there can be errors in the higher values of variance this can lead to the multiple errors where there is high variance and low accuracy It can result in very different results and the actual results may vary hugely depending upon the  Variance on the area of the graph"
      ],
      "metadata": {
        "id": "N2diNdLvg7wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12)How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "To improve a multilinear regression with a very high multi coloniality generally a multicolinearity means two or more input variables are dependent upon each other change in one input variable can result change in another input variable this multiple dependence on input variables can give a very wrong estimation of the output and can mess with the regression coefficient to solve this multicolinearity we generally have two methods\n",
        "\n",
        "RFE recursive feature elimination\n",
        "\n",
        "Generally in this method one feature is removed and the output is calculated and iteratively an individual feature is removed in the output is calculated in the process we will check the dependence and the contribution of the input variable to the output variable\n",
        "\n",
        "VIF variants inflation factor VIF is generally calculated using R Square metrics generally VIF is calculated for every feature when calculated or testing of the other features with the input variable the VIF of more than ten indicates that the 90% of the feature is explained by the all other features this can lead to the multicolor linearity rimso removal of this feature can greatly improve the results and optimize the algorithm\n",
        "\n",
        "There are two kinds of multicolinearity 1 is data based multicolinearity the second is structural based multi-olearity the data based multi-collarity is generally dependent upon the raw data and we can use these methods to remove the tagolivrt\n",
        "\n",
        "Structural based multi polarity this is caused due to the new features added from the existing features when two or more features can be combined to create an existing features and the old feature are not remote this can lead to the multicore tip this can be eliminated using the removal of the old features which have been combined into the new features this can reduce multicolinearity significantly"
      ],
      "metadata": {
        "id": "6LryPlT-hR2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13)What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Some of the common techniques for transforming category variables for use in regression mode models depends upon the what are the categorical variables generally there are three methods 1 is ordinal nominal target in target guided ordinal encoding\n",
        "\n",
        "In ordinal encoding generally there is order or a priority in the categorical variables this can be marked as 1234 each every variable has a priority over the other variable examples include high medium and low pass or fail good better and best\n",
        "\n",
        "In nominal encoding generally there is no order our priority in the category variables this can be marked as one hot encoding which uses multiple columns of zeros and ones for a particular variable a single variable is divided into three or 4 variables depending upon the number of or different kinds of variables this one or zero is generally added to the table at the original column is removed this converts category variables into numerical variables\n",
        "\n",
        "In target guided ordinal encoding we can combine two category variables depending upon current situations for example when an ordinary we check the relationship with large variables and it is useful to have very large number of unique categories we can directly mark those two variables and calculate the average to find the relationship between the 2 columns these 2 columns can be combined and ordinal encoding can be used accordingly in the regression model"
      ],
      "metadata": {
        "id": "jSIRVdLHiDd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14)What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "The role of interaction terms in multiple linear regression is used to analyse relationship between the input variables and output variables how a particular input variable has an influence or changes the value of an output variable it is also calculated the change in each individual variable and the amount of effect on the overall output variable this interaction terms is used to calculate the weights of every input variable to a particular output variable"
      ],
      "metadata": {
        "id": "Orf7ljX_iwBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15) How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "Interception of intercept before between simple and multiple linear regression and simple linear regression they intercept is a singular value where it adds a single bias but in multiple linear regression it has an overall bias to all the input variables combined so it is calculated after addition of all multiple input variables with their particular weights in simple linear regression we directly add it"
      ],
      "metadata": {
        "id": "ZR_8PygCrakc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16)What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Generally the slogan regulation analysis defines the amount of change in input variable with respect to the output variable it defines the influence and dependency of output variable to the respective input variable in simple linear equation it generally calculated as a slope and has a single value if the slope is one then you say when the change occurred in X is the same as same as change occurred in Dubai if it is more than one then there is a large number of change in Y with respect to X similarly multiple linear regression analysis every input variable has a respective slope multiplied total this actually defines the dependency and the influence of input variable"
      ],
      "metadata": {
        "id": "NUbA8i1jrvse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17)How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "The intersect in a regression model provides a context between the variables in certain way the intercept usually provides data that when there is zero when the input rail goes over 0 what could be the expected output variable it is a non 0 with input variables everything is zero then the output is usually equal to the interceptor it is usually choose to add a bias or adjust the result according to the output\n"
      ],
      "metadata": {
        "id": "CeatU7bCsKfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18) What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "\n",
        "The main limitations of R2 as solemn measure of model performance it is insensitive to the outliers if the outliers can cause overfitting or misinterpret the world completely\n",
        "\n",
        "The second thing is it is only a measure for linear relationships if there is a non linear relationship between the data inputs and Outputs then it is a wrong metric to measure the performance of a model\n",
        "\n",
        "It ignores bias and reduced patterns which can cause many misleading and can limit the use of R Square for the models performance\n"
      ],
      "metadata": {
        "id": "Gpj_KYbrsX-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19) How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "To interpret large stranded rf for recreation coefficient this can be caused to the many reasons one of the following main reasons is multi coal in reality in this reason there can be two or more features which are dependent upon each other that means two are more free change in one feature also has a significant change in our features in the 90% or 80% of the feature can be explained by the other features this can causeway a large standard error\n",
        "\n",
        "The second thing is that can happened when we interpret a large standard error or not normally distributed along the best fit line or the mean of the expected line this means it can have a little bit of non linear relationships and errors are not distributed along the best fetline the third thing is\n",
        "\n",
        "The third thing is the patterns are linear in a very different way they require one or two more models to explain the complete patterns in the data this can be understood by large standard error"
      ],
      "metadata": {
        "id": "u9p_AvVbVKUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20)How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Idrocity can be identified in residual plots by identifying the change in variance across various inputs and Outputs this change in variance of the distribution of errors along the various input s can occur if the data is heterocyclic and it is not ideal for a regression this can happen in the form of various clusters on non random patterns or in a bow shade pattern or in a residual vs fitted plot\n",
        "\n",
        "\n",
        "It is important to address heterogeneity As it affects confidence intervals which can be too wide or narrow or it can reduce the model efficiency completely this can be very one sided and can no longer fit the best fit line making the model useless it can also has a great effect on prediction accuracy this means if the variance is certain at one part of the inputs and uncertain at another part of the input then the results can vary for depending upon each input and the event error is not normalised along the best fit line then it can causeway many problems in prediction of the data or output"
      ],
      "metadata": {
        "id": "v7C4ih_gVzn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21)What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "\n",
        "Usually if a multi remedial model has a high R ^2 and low adjuster R Square this can cause due to the multicolority whereas certain feature or an input can be explained using the other features second thing is this can happen due to the presence of irrelevant predictors or which can overall causeway over fitting r Square increases as the accuracy increases but when there are unnecessary features or features which do not have too much impact or contribution in the output then there is a large gap between R Square and adjusted R Square usually the difference between adjusted and R Square and R Square should not be more than 5% if it is more than 5% then there can be a problem this can also happen when the sample size is too small and there are too many input variables which can causeway poor generalisation of the model the model may underfit"
      ],
      "metadata": {
        "id": "RRVdjmbIWf9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 22) Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "It is very important to scale variables in multiple linear regression because various inputs can have various different scale or ranges for example one feature can have its scale in years and it can have very large numbers another scale can have like a sr no question which can be interpreted as 0 and 1 it has a very small scale features this difference in the range and the plus of the number should not have an impact while deciding the output of a variable it can improve the interpretability of the coefficient this scaling allows all the inputs to be equal relative to each other this can avoid multi goal linearity and includes all inputs in a good way it can also help with gradient boosting or regularisation models some of the common scaling methods are standardisation of these core scaling this can be used when the data is have normal distribution curve or it can be converted to normal distribution using central limit theorem or we can even use min max scaling in which the number is calculated by subtracting with the minimum value of the feature and dividing it with a range of the feature"
      ],
      "metadata": {
        "id": "emI4z1nLW9AF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 23)What is polynomial regression?\n",
        "\n",
        "Polynomial regression is usually import ant whenever we have a curves in the data usually simple or multilinear regression reducing assumption that the all data or the all data points are linear in every way they are **** scadastic but in polynomial regression this allows for non linear shape of data this can happen when we square or cube the numbers this allows it to add an extra dimension in a new way of curve it can be used to describe non linear relationships then can increase accuracy for non linear data the polynomial regression can be identified using a max plus M1X1 square plus M2X2 square plus M3 X3 square it is very useful described non linear data"
      ],
      "metadata": {
        "id": "JVVhn4-IXkyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 24) How does polynomial regression differ from linear regression?\n",
        "\n",
        "How does polynomial regression differ from linear regression usually in linear regression whether it is simple or multiple linear reduction the main assumption is that all the data points are linear or straight in line or it can be fitted in a straight line the best straight line is usually a straight without any curves and all the errors are normal distributed along the best fitted line this is a major assumption in the linear regression R but in polynomial regression we add an extra assumption we remove the assumption that the all the output variables are linear when we remove we can add non linear relationships to data which can actually help improve the prediction and accuracy of their non linear data types this can be done by using the curves and to use the curves we can use square cubes and extremely power 4 and power 5 this can be used to create a curve and improve the accuracy of the model even though the data is non linear in nature"
      ],
      "metadata": {
        "id": "DE8qlDnwX9SN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 25)When is polynomial regression used?\n",
        "\n",
        "Polynomial regression is generally used when the data is nonlinear or when the non linear assumption is generally removed when we do not know that the data is linear or nonlinear or the nature of the data we can use the polynomial regression generally polynomial regression is used to define non linear relationships in data that actually includes curves whether rather than when we observe a straight line in linear regression this suction is very useful to identify various data types in the nature of the data"
      ],
      "metadata": {
        "id": "ySkEFOwyYUJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 26) What is the general equation for polynomial regression?\n",
        "\n",
        "y=Î²0+Î²1âx+Î²2x2+Î²3âx3+â¯+Î²nâxn+Ïµ"
      ],
      "metadata": {
        "id": "NNozeSRHYiGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 27)Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Polynomial regressions can be applied to the multiple variables same number of ways just by increasing the input such as X1 X2 X3 rather than X1 square or X2 square or X2 cube adding the multiple variables in regression can increase complexity therefore while increasing the number of variables are by using it on multiple variables its better to test it on linear vibration and then to increase accuracy we can test it on polynomial regression\n",
        "\n"
      ],
      "metadata": {
        "id": "aqKkmRQCcl3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 28)What are the limitations of polynomial regression?\n",
        "\n",
        "The major limitations of polynomial regression are when there are too many cops or when the data has too much randomness then the polynomial regression can over fit the data or memorise the data completely so when the new data is actually asked then it can give very inaccurate predictions one of the major impression of polynomial regression is it can increase the complexity as the data becomes more and more nonlinear or randomness one of the biggest limitations of polynomial regression is it assumes that all the data on the best fit line the errors are normally distributed along the best fit line if the data has hetero scarcity then the polynomial regression may or may not be able to predict the right results or outputs"
      ],
      "metadata": {
        "id": "muYKKIU3c4bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 29)What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "One thing can be residual analysis actually checking the QP plot with the residual following a normal distribution or not or checking the **** scaracity the other thing is goodness of the fit which means we can calculate R Square and adjuster R Square this R Square and adjust square R Square should not have more difference between more than 5% their difference is more than 5% then there are too many irrelevant features or it can causeway multicolor third thing is we can use the cross validation methods where we can use splitting the data and we can use the various methods such as leave one out cross validation leave K out cross validation the third thing is regularisation we can use L1 or lasso rigid techniques which can shrink coefficient to prevent overfitting the last thing is we can visualise the data if the data is within the two dimensional 3 dimensional when the dimensions are more than 3 then it becomes harder and harder to visualise data"
      ],
      "metadata": {
        "id": "kXyr-v1PdNzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 30)Why is visualization important in polynomial regression?\n",
        "\n",
        "visualisation is important polynomial regression generally to cheque if the best fit line needs any curves or not if the best fit line actually needs a non linear relationship between the data then we need to visualise properly whether teaser polynomial regression or not the linear relationship in the data can be usually checked using QQ Plot and the normalisation of errors around the best fit line is usually checked by visualisation this can also be used to cheque homosecity before using any polynomial regression it is much easier to use by checking the actual output and deciding to what degree do we need to use the polynomial regression or whether not to use the polynomial degression since the polynomial regression the complexity of the model can be increased that is why it is visually important to analyse the data before using any of the coefficients"
      ],
      "metadata": {
        "id": "HWb09Ja-duzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 31) How is polynomial regression implemented in Python?\n",
        "\n",
        "The polynomial liberation is implicated in python in a similar way we implement linear regression first we have to import polynomial features from sql and preprocessing then we have to create an object for polynomial features in this object we have to pass generally two basic parameters 1 is degree another is included by us the degree generally means how many curves or how many times the curve should actually change where the degree is to then usually the quadratic graph will be formed and many degrees 3 the cubic graph will be formed according to non linear relationships or the changes in the output of the data we have to select the degree this can be done by visualising the data include bias is usually false after creating this object we have to fit transform the extraneous part of the data after that we can directly import linear regression and we can directly import the poly fit train and wide train data"
      ],
      "metadata": {
        "id": "X1I8DFwseGFc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qH_UG1DdcIKT"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}