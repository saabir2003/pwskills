{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt/hBFYmAaOJrcy76UlK/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saabir2003/pwskills/blob/main/Feature_Engneering_PWSkills.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W4qRLcvaA2jv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "#filterswarnings(ignore='True')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1)What is a parameter?"
      ],
      "metadata": {
        "id": "Z4A0yTs0Bb4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Parameters are the value decided by the  model depending upon the inputs and the given target variable outputs the parameter is chosen by the model and it is optimised iteratively to get the most optimal result\n",
        "\n",
        "There are different parameters for different types of machine learning models for example in linear regression the parameters are coefficients of X1 Y 1 and Z1 of the input variables are dependent variables and they intercept these are known as parameters in linear regression in the same way they are multiple parameters for various types of regression classification other problems\n",
        "\n",
        "Theres a lot of difference between parameters and hyperparameters both can be confused for each other hyperparameters are the adjustments are manual tuning of the learning rate etc of the model which can be adjusted by the developer whereas parameter are internally belong to the model decided by the machine learning algorithm depending upon the inputs and Outputs hyperparameters are tuned to get the most optimal parameter in the machine learning algorithm"
      ],
      "metadata": {
        "id": "Pg_Q0wehCRxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2)What is correlation? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "zWwRU6tuC00m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation is an idea to identify relationship between two or more variables it is used to determine the relationship and dependency between the two variables if the two variables are highly correlated then the change in one variable can lead to equal change in the other variable for example salary and age these two Teach two variables are highly correlated because change in one variable can result in change in other variables increase in age or salary will increase in every other variable there are different types of correlations this can be positive correlation or negative correlation\n",
        "\n",
        "\n",
        "The correlation general range can be minus one to 1 the 1 between two variables implicates that the increase in one variable results in decrease in other variable in a similar way the correlation of one represents increase in one variable also increase in another variable if the correlation is zero then both the data points are the variables have no correlation or relationship among them and can be randomly distributed.\n",
        "\n",
        "\n",
        "There are various types of correlation formulas which can be used for parametric test and non parametric test a parametric test is correlation formula is used when all the distributions are linearly dependent some of the parameter correlation types are Pearson correlation with a range of one to 1 it analyses relation between variables which can measure real relationship between two variables another type of non parametric test for correlation analysis is spheremen's rank correlation it uses ranks of the data node the absolute raw data to analyse the coordination between the 2 values both have different formulas another type of correlation is point by serial correlation it defines relationship between the two different types of target variable and a metric variable the last type of correlation is the calls rank correlation this is also non parametric test the data doesn't need to be normal distributed and the two variables need to have only ordinal scale variables this can be used in case of non parametric.\n",
        "\n",
        "A negative correlation means that When a variable increases it can result in significant decrease of another variable"
      ],
      "metadata": {
        "id": "-iimU8o-C4-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3)Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "8eajg3q5EwKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Machine learning focuses on creating algorithms and statistical models to prepare or make predictions there are generally four types in machine learning which can be supervised machine learning unsupervised machine learning semi supervised machine learning and reinforcement machine learning\n",
        "\n",
        "Supervised machine learning training may moral depending upon the labelled data unsupervised machine learning training depends upon the unlabeled data .\n",
        "Semi supervised machine learning refers to the use of Supervest and unsupervised machine learning models and reinforcement machine learning depends upon the reward and punishment model in which the model is rewarded depending upon the target.\n",
        "\n",
        "There are many different types of machine learning algorithms they can be linear regression, Loyistic regression predictions lesser regression nervous K Indian's decision trees etc\n",
        "\n",
        "\n",
        "The different components in machine learning include the data, the model ,the hyperparameter tuning ,the loss optimization function, target variables,"
      ],
      "metadata": {
        "id": "Pi-LWg7AFH8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4)How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "f8fUPwkoGJGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The loss value help in many other model or is good or not in many different ways it measures in model error which can be mean absolute error mean root mean squared error in linear regression and in logical regression it helps in guiding model training such as in linear regression it can help to calculate the cost function in gradient descent to guide the model to an optimal parameters.\n",
        "\n",
        "#### Loss can be used to compare between different models such as decision free linear regression and models with various hyperparameters this can be used to evaluate which model has less loss which can be picked up this can result in very high accuracy and it can be measured to calculate the precision F1 recall of a model\n",
        "\n",
        "#### It can be used to avoid overfitting if the training loss is slow but the validation loss is high the model is likely over fitting this can be used to cheque if the model is overfitting and take appropriate steps to avoid overfitting\n",
        "\n"
      ],
      "metadata": {
        "id": "KMQrOA_cGL2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5)What are continuous and categorical variables?\n"
      ],
      "metadata": {
        "id": "g8AERWFLG56A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The continuous variables are usual the numerical variables which can have integers are float values the category values are usually non numerical values such as yes or no true or false or a situation where the person has to choose between two or different amount of non numerical objects.\n",
        "\n",
        "#### The continue variables are usually the numbers Which can be defined in a certain range of values whereas in categorical variables we have to choose between various options which can be true or false there are different types of category variables nominal and ordinal in nominal there is no order between the variables which can be for example hot sunny rainy in ordinal categorical variables there is a particular order or order of priority the variable such as high medium or low the continuous and categorical valuables have different scaling functions and mathematical functions to scale and normalise the values the missing continuous variables use mean or median to impute the missing values in category valuables we use mode"
      ],
      "metadata": {
        "id": "9txjJ6lCG9dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6)How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "de9MqimAHfgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To handle categorical variables in machine learning there are multiple methods the categorical variables can be defined in the form of various types which can mean nominal or ordinal in nominal types there are no orders between the categorical variables this can be a unique variables such as sunny hot and rainy in ordinal categorical variables there are certain order in the value surgery objects in categories for example high medium and low to handling the missing values in the category of variables we can use the model function to calculate the most frequent categorical variable and impute the value accordingly\n",
        "\n",
        "#### For data encoding the categorical variables there are three different types this is nominal or 1 hot encoding the 2nd 1 is label and ordinal encoding the 3rd one is target guided ordinal encoding in nominal are one hot encoding we use dummy variables function in the pandas to define a value for every unique type this can be represented in the form of zeros and ones if the disadvantage is this can increase complexity of the complex has many many categories the second is label or ordinal encoding in which we assign a numerical data to each category this can have no curse of dimensionality and disadvantage it is good for ordinal data when we consider all level of data the third one is target gathered encoding where depending upon the relationship with the target variable we can rewrite the category variable into numerical types\n",
        "\n",
        "####  common techniques are:-\n",
        "Common techniques for treating missing values is using more for data encoding there are three types of techniques which can be\n",
        "\n",
        "1.   **nominal/One Hot encoding**\n",
        "2.  **Label and ordinal**\n",
        "3. **Target guided ordinal encoding**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F2zS0YK4Hh1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7)What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "GiXqLNmLJUt-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Every data set before any model creation or data analysis is separated by training and testing data set this training data set is used to train the model and tune the hyperparameters according to the model after tuning and model creation the same model is tested using the tested set of the data set this is used to cheque the accuracy and cheque if the model is under fitting or over fitting if the model is overfitting the process is started again from the beginning to create the correct model for the use case\n",
        "\n",
        "#### Usually the testing and training ratio is between 0.2 to 0.8 or 0.25 to 0.75\n",
        "#### The training data can be further rewarded into training set and validation set for the training set is used to decide the parameters and the validation set can be used to decide the hyperparameters for the model these hyperparameters for the model can be tested and validated by using the testing data set this is useful in detecting overfitting and creating the correct model for an exact prediction in good accuracy of the model"
      ],
      "metadata": {
        "id": "Nt7r6jRlJZ7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8)What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "s383v2xWKFw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### sklearn preprocessing is a type of library which can be used in python this library is used to do all preprocessing tasks and data manipulation\n",
        "1.   scaling and normalization\n",
        "2.   encoding categorical variables\n",
        "3.   handling polynomial features\n",
        "4.   Transformations\n"
      ],
      "metadata": {
        "id": "TAlqbuI9KIhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9)What is a Test set?"
      ],
      "metadata": {
        "id": "k23QjAgoK7ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A test set in machine learning is used to cheque the accuracy of the model to avoid overfitting and detect the accuracy of the model we can use several metrics to To identify the accuracy model which can be F1 score recall precision etc the test set is usually in the ratio of 0 .2 to the total data set this can be also used to validate the hyperparameters and cheque the actual accuracy."
      ],
      "metadata": {
        "id": "QhDOdGG3LTI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10)How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "##How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "H29CrkyzLolN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display data for model fitting in Python in this way first important test split from more scale learn model selection library after importing we create an object of train test split set the input and output variables and choose the test size in which we include the ratio of 0.2 to 0.25 this object returns four partitions of the data exchange wide range extest Y Test which can be respectively used to train and test data according to the input and output variables\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,y_train,x_Test,y_test=train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "To approach emotional learning problem there are several steps the first step would be to load the model the second step would be data exploration in data cleaning in which we identify null values impute those values cheque if there is an imbalance data sets in an target variable cheque if the data set is clean by looking at the categorical and numerical columns next we do eda which is known as exploratory data analysis in which we do feature engineering and feature selection where we analyse all the data and its relationships to each other as well as to the target variable after eda we do model selection and model building in which we select an appropriate model by looking the stats at the eda while in model building we choose the appropriate model depending upon the use case and divide the data set the training part of the data set is used to train the model and we hypertune the model to get a good result in the testing data set this can be also used to avoid overfitting after getting the test results we can get the accuracy and the classification report for machine learning problem"
      ],
      "metadata": {
        "id": "ipDwYlO7Lq55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11) Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "ahlRMkmZNW1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eda are exploratory data analysis is a critical step before training a machine learning model it is usually useful to understand the data patterns and which model to use before fitting the model it can be used to understand the data the relationship between each other and the target variables it is very important to do univariate bivariate and multivariate analysis between the various relationship and construct a heat map to do correlation analysis.\n",
        "\n",
        " data distribution\n",
        "\n",
        "It is used to detect missing values and impute them accordingly for a continuous values we can use or impute the missing values depending upon the mean and more for categorical values we can impute the mod values\n",
        "\n",
        "identify outliers\n",
        "\n",
        "We can identify outliers using inter quartile range and by using boxplot or violin plot this can be used accordingly to identify outliers the number of outliers can be used easily to clear all the outliers\n",
        "\n",
        "encoding\n",
        "\n",
        "Can use this to encode the categorical data this can be done using nominal category data encoding or ordinal categorical data encoding the nominal data encoding can be done easily by using the dummy function in the pandas which creates column for every unique value in a particular column the ordinal categorical data refers by assigning a numerical value to the particular unique column\n"
      ],
      "metadata": {
        "id": "rdWpyX95NfCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12)What is correlation?"
      ],
      "metadata": {
        "id": "hPed_S5jOVfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coalition refers to the dependence or relationship between two or more variables the correlation can be measured using various tests such as using Pearson or spearmint correlation analysis the two variables are features are set to be highly correlated if the change in 1 variable also increases the value in another variable The correlation has a range of one to 1 where one refers to the negative correlation and one refers to the positive correlation and zero refers to the number correlation native correlation refers that the two values are inversely correlated which means an increase in 1 variable can result in decrease in another variable positive correlation refers to the increase in one variable will also causeway increase in another variable 0 or no correlation refers to the random points are randomly distributed and there is no proper correlation\n",
        "\n",
        "df.corr()"
      ],
      "metadata": {
        "id": "MCYxGajSOYmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13)What does negative correlation mean?"
      ],
      "metadata": {
        "id": "jf5QpN2tOw_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative correlation refers that two variables are inversely proportional it means that increase in one variable results in decrease of another variable the value for highest negative correlations in Pearson is -1"
      ],
      "metadata": {
        "id": "r206uKSCOzD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14)How can you find correlation between variables in Python?\n"
      ],
      "metadata": {
        "id": "eszOPVKaPAjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate correlation between the variables there are different types of ways or tests for variables with linear dependency we use parametric test which can be used to spearmen correlation whether ranges between -1 to 1 for nonparametric test that is for non linear python data sets we can use spearman coefficient tester where we use the ranks of the data then the raw data to calculate the correlation\n",
        "\n",
        "\n",
        "using pandas corr() function\n",
        "df.corr(method='pearson')\n",
        "df.corr(method='spearman')\n",
        "df.corr(method='kendall')\n",
        "\n",
        "\n",
        "using seaborn heatmap\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "using numpy for pairwise correlation\n",
        "\n",
        "\n",
        "correlation = np.corrcoef(df['A'], df['B'])[0, 1]\n",
        "\n",
        "\n",
        "\n",
        "using scipy for significance\n",
        "\n",
        "corr, p_value = pearsonr(df['A'], df['B'])"
      ],
      "metadata": {
        "id": "rjjKwps0PCZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15)What is causation? Explain difference between correlation and causation with an example"
      ],
      "metadata": {
        "id": "iS-rFtVjQFFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation means one event directly causes another event to happen or it influences the other event to happen directly\n",
        "\n",
        "Corelation means two events are related but a certain event does not directly cause another event to change\n",
        "\n",
        "Causation example is the sale of ice cream and drowning victims since summer\n",
        "\n",
        "In summer both the ice cream sales and swimming activity increases which can cause drowning but this doesnt mean that the sale of ice cream directly depends upon the drowning victims this is an example of causation.\n",
        "\n",
        "\n",
        "Corelation example is number of hours exercised and the age up to which the person lives\n",
        "\n",
        "In this example the more a person exercises the more age up to which a person can directly influence the age and health of the person and which can result in longevity this is example of correlation"
      ],
      "metadata": {
        "id": "WkAuYVWyQHyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16)What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "SniMhy9URLfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm or a method which is used to modify or change the model parameters which are weights and bases it is used to minimise the loss function to increase the overall accuracy of the model it is used to update parameters based upon the loss function calculated during the pack propagation\n",
        "\n",
        "\n",
        " Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Different types of optimizers include stochastic gradient descent which is also known as Sgd this is used to update regular weights by taking small steps in the direction of the negative gradient descent we use learning rate in this this is optimal and is simple and efficient\n",
        "\n",
        " RMSprop (Root Mean Square Propagation)\n",
        "\n",
        " This is adopts the learning rate for each parameter by reconsidering the recent gradient descent it works well for non stationary platforms and it is used for deep learning networks example RL it can be unstable without proper tuning\n",
        "\n",
        " Adam (Adaptive Moment Estimation)\n",
        "\n",
        " This is mainly used in deep learning neural networks this combines momentum and RM stop for faster and more stable convergence it is very fast and handle sparse gradients well this is mainly used in convolutional neural network and other deep learning tasks such as image classification sentimental analysis object detection etc"
      ],
      "metadata": {
        "id": "3EPK-_08Rbyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17)What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "zWPVxvvhSL4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skelon linear model is a model in circuitline that provides various models for linear regression and classification tasks some of the models that are available in this are linear regression lesser regression or rigid regression for classification task there is logistic regression this is useful for predicting continuous values and it fits a line to minimise the sum of squared residuals this is mainly used for data that has errors which is normal distributed over the predicted line in training data set it's a simple and easy to use library"
      ],
      "metadata": {
        "id": "YIUZfZvJSSDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18)What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "R3H0JEVfSix9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model.fit() It is used to train any data or model depending upon the data it cheques the input data and the target variable the required arguments are X and Y where X is the input variable and Y is a target variable these are the two main arguments when this function is run it cheques the input data computes gradient descendants optimised parameters and stores learn parameters in the model for future predictions\n",
        "\n",
        "The arguments that should be given is mainly X And why were X is an input variables and Y is a target output variable epochs is generally used in neural network and size is also used in the neural network generally in machine learning x and Y is basically enough"
      ],
      "metadata": {
        "id": "Is3aayavSk0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19)What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "NvaFOj9PTGSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model.predict() This method is used in scikit learn to make predictions using a trained model the trained model is trained using the model outfit function as I have described in the above the arguments that should be given in the moral predict function is xtest which is testing input variables which are not usually used while training the model\n",
        "\n",
        "This method arguments are x test after the prediction of the extras this value size actually stored in Ypred the predator values are compared in the accuracy is calculated"
      ],
      "metadata": {
        "id": "FAllpKqvTIsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20)What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "MgyNmNZyTlXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous variables are integer variables which are very large range and categorical variables are unique object variables\n",
        "\n",
        "The examples of continuous variables is numbers are float failure points the examples of category variables is true or false yes or no zero or 1 etc\n",
        "\n",
        "There is only one type of continuous variable but there are two types of category variables the 2 types of category variables are nominal category variables and ordinal categorical variables in nominal categorical variables there is no order between the categories of data for example in whether feature they can mute all the attributes are equal such as Sunnyvale Rainy Windy etc but in ordinal categorical variables there is certain order in the data for example high medium low"
      ],
      "metadata": {
        "id": "TV8R7nZcTnGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21)What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "jN74GXRFUAie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling it is used to normalise all the data between the zero and one there are two ways to do feature scaling for numerical variables 1 is to do standard scalar in which all the values are between 0 and 1 another is normalisation in which you use normal distribution formula to convert all the values in a normal distribution form\n",
        "\n",
        "For categorical variables we can use feature scaling separate for nominal and ordinal variables for nominal variables we can use dummy variables in Pandas function and for ordinal feature scaling we can use certain numbers to convert all the orders of the data into numerical values\n",
        "\n",
        "It helps in machine learning to preprocess the data in a much faster way and detect any outliers in test data it helps in equal feature dependence or effect on the machine learning model rather than have one feature dependent on the machine learning model more than the other this can be also used to detect the range of the values while using the test data and to predict to get an accurate results"
      ],
      "metadata": {
        "id": "6LQHp_VzUCmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 22)How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "G7LvlCfjUhpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can perform scaling differently for categorical variables and continuous variables for continuous variables we can use standard scalar method or normalisation in standard scalar method all the values in the feature column is defined or normalised between zero and one in normalisation all the values are rearranged in normal distribution in the between the mean with the standard deviation of 1\n",
        "\n",
        "To perform scaling on categorical variables there are two types of categorical variables which can mean nominal categorical variables where there is no order between the values in a feature column for example in whether sunny rainy windy these all don't have any order or priority in between them for this we can use dummy variables function or one hot encoding but in an ordinal variable there is a certain order between the values in a feature table for this we can use label Encoding in which for every feature or priority we use a numerical replacement with this we can perform scaling\n",
        "\n",
        "from sklearn.preprocessing import standardScaler,OneHotEncoding\n",
        "\n",
        "one=OneHotEncoding()\n",
        "x_train=one.fit_transform(feature)"
      ],
      "metadata": {
        "id": "hIANpircUj8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 23) What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "T9nDC4tqVBPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skelon.preprocessing is a model that contains various functions to perform scaling normalisation and other data manipulation tasks for scaling this can we can import standard scalar min max scalar or normalisation for continuous variables for categorical variables we can import one hot encoder or label encoder or dummy variables this is very useful as it contains function to preprocess data"
      ],
      "metadata": {
        "id": "dUqydw_BVF8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 24)How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "WvfCgZEYVYsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split data for model fitting using train test split by importing it in from sql on dot model selection model we split train and data by making object of the train test split and the ratio in which we split the train test is 0.7 into 0 .3 or 0.8 to 0 .2\n",
        "\n",
        "from sklearn.model_selection import train_test_split()\n",
        "\n",
        "x_train,y_train,x_Test,y_test=train_test_split(x,y,test_size=0.2)\n"
      ],
      "metadata": {
        "id": "-Lal3nC5VdFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 25)Explain data encoding?"
      ],
      "metadata": {
        "id": "RjQ4OvY1VrrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is a process of converting category data into numerical formats where the machine learning algorithm can understand and preprocess to train the data accordingly this is done in the categorical data there are many ways to do data encoding\n",
        "\n",
        "The types of data encoding which include label Encoding one hot encoding for Target Encoding\n",
        "\n",
        "Label Encoding is used for ordinal data where every data is has a priority such as high medium or low\n",
        "\n",
        "One hot encoding is used in ordinal data where every data is equal and has no relationship between them such as sunny rainy windy etc\n",
        "\n",
        "Target encoding is used by combining a numerical column we use it in usually use it in one hot encoding where we compare with numerical column and use the mean to replace the values are combined to columns"
      ],
      "metadata": {
        "id": "FGMTmfH8V01K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dTednZB9LkVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}